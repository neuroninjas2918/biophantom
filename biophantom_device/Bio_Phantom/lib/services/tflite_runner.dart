import 'dart:typed_data';
import 'dart:math' as math;
import 'package:tflite_flutter/tflite_flutter.dart';
import 'dart:io' show File;
import 'package:flutter/services.dart' show rootBundle;
import 'fusion_config.dart';
import 'calibration.dart';

class _QInfo {
  final List<int> shape;
  final TensorType type;
  final double scale;
  final int zeroPoint;
  _QInfo(this.shape, this.type, this.scale, this.zeroPoint);
  factory _QInfo.of(Tensor t) {
    final q = t.params;
    final s = (q != null) ? q.scale : 1.0;
    final zp = (q != null) ? q.zeroPoint : 0;
    return _QInfo(t.shape, t.type, s, zp);
  }
}

class TfliteRunner {
  Interpreter? _vibInterpreter;
  Interpreter? _audInterpreter;
  late final _QInfo vibIn, vibOut;
  _QInfo? audIn, audOut;
  Calibration? _calibration;
  bool _useAudioFallback = false;

  TfliteRunner();

  Future<void> _initializeVibInterpreter(int vibT, int vibC) async {
    if (_vibInterpreter == null) {
      final config = await FusionConfig.load();
      _calibration = await Calibration.load();

      _vibInterpreter = await Interpreter.fromAsset(
        'assets/models/biophantom_motion_int8.tflite',
      );

      _vibInterpreter!.resizeInputTensor(0, [1, vibT, vibC]);
      _vibInterpreter!.allocateTensors();
      vibIn = _QInfo.of(_vibInterpreter!.getInputTensor(0));
      vibOut = _QInfo.of(_vibInterpreter!.getOutputTensor(0));
    }
  }

  Future<void> _initializeAudInterpreter(int audT, int audC) async {
    if (_audInterpreter == null) {
      try {
        print('üéµ Loading audio model: biophantom_audio_int8.tflite');

        // Create interpreter options with basic CPU execution
        final options = InterpreterOptions();
        options.threads = 4; // Use 4 threads for better performance
        print('üéµ Using CPU execution with 4 threads');

        _audInterpreter = await Interpreter.fromAsset(
          'assets/models/biophantom_audio_int8.tflite',
          options: options,
        );

        print('üéµ Resizing audio input tensor to [1, $audT, $audC]');
        _audInterpreter!.resizeInputTensor(0, [1, audT, audC]);
        _audInterpreter!.allocateTensors();
        audIn = _QInfo.of(_audInterpreter!.getInputTensor(0));
        audOut = _QInfo.of(_audInterpreter!.getOutputTensor(0));

        print('üéµ Audio model loaded successfully!');
        print('   Input shape: ${audIn!.shape}, type: ${audIn!.type}');
        print('   Output shape: ${audOut!.shape}, type: ${audOut!.type}');
      } catch (e) {
        print('‚ùå Failed to load audio model: $e');
        print('‚ùå Stack trace: ${StackTrace.current}');
        print(
          '‚ö†Ô∏è Audio model uses unsupported TensorFlow operations. Using fallback simulation.',
        );
        _audInterpreter = null;
        audIn = null;
        audOut = null;
        // Set a flag to indicate we should use fallback audio analysis
        _useAudioFallback = true;
      }
    }
  }

  Future<double> runVibrationWindow(List<List<double>> window) async {
    final config = await FusionConfig.load();
    await _initializeVibInterpreter(config.vibT, config.channels);

    List<List<double>> processedWindow = List.from(window);
    if (_calibration != null) {
      processedWindow = _calibration!.zVib(processedWindow);
    }

    final u8 = _q2D(processedWindow, vibIn.scale, vibIn.zeroPoint);
    final input = _nest(u8, vibIn.shape); // [1,T,2]
    dynamic out;
    if (vibOut.type == TensorType.uint8) {
      out = List.generate(1, (_) => List<int>.filled(1, 0));
    } else {
      out = List.generate(1, (_) => List<double>.filled(1, 0.0));
    }
    _vibInterpreter!.run(input, out);
    return _deq(out, vibOut);
  }

  Future<double?> runAud(List<List<double>> t2) async {
    final config = await FusionConfig.load();

    // If using fallback (model failed to load), simulate audio analysis
    if (_useAudioFallback) {
      print('üéµ Using fallback audio analysis (model not compatible)');
      return _runAudioFallback(t2);
    }

    await _initializeAudInterpreter(
      config.audT,
      1,
    ); // Audio models typically use 1 channel (mono)

    if (_audInterpreter == null || audIn == null || audOut == null) {
      print('‚ùå Audio model not available');
      return null;
    }

    print('üéµ Running audio inference with ${t2.length} time steps');
    List<List<double>> processedWindow = List.from(t2);
    if (_calibration != null) {
      processedWindow = _calibration!.zAud(processedWindow);
    }

    final u8 = _q2D(processedWindow, audIn!.scale, audIn!.zeroPoint);
    final input = _nest(u8, audIn!.shape);

    print(
      'üéµ Input tensor shape: ${audIn!.shape}, quantized values sample: ${u8.take(5)}',
    );

    dynamic out;
    if (audOut!.type == TensorType.uint8) {
      out = List.generate(1, (_) => List<int>.filled(1, 0));
    } else {
      out = List.generate(1, (_) => List<double>.filled(1, 0.0));
    }

    try {
      _audInterpreter!.run(input, out);
      final result = _deq(out, audOut!);
      print('üéµ Audio inference result: $result');
      return result;
    } catch (e) {
      print('‚ùå Audio inference failed: $e');
      return null;
    }
  }

  // Fallback audio analysis when model is not compatible
  double _runAudioFallback(List<List<double>> audioData) {
    print('üéµ Analyzing audio patterns for cough detection...');

    // Analyze the audio data for cough-like characteristics
    double coughProbability = 0.0;

    // Look for cough patterns: sudden spikes, harmonics, decay
    for (int i = 0; i < audioData.length; i++) {
      double db = audioData[i][0];

      // Cough detection based on dB patterns
      if (db > 70.0) {
        // Loud sounds
        coughProbability += 0.3;
      } else if (db > 60.0) {
        // Moderate sounds
        coughProbability += 0.1;
      }

      // Look for explosive onset (sudden increase)
      if (i > 0) {
        double prevDb = audioData[i - 1][0];
        if (db - prevDb > 10.0) {
          // Sudden 10dB+ increase
          coughProbability += 0.2;
        }
      }

      // Look for harmonics (multiple frequency components)
      // This is simulated since we don't have frequency analysis
      if (db > 50.0 && i < audioData.length - 1) {
        double nextDb = audioData[i + 1][0];
        if (nextDb > db * 0.7) {
          // Sustained energy
          coughProbability += 0.1;
        }
      }
    }

    // Normalize and add some randomness to simulate model uncertainty
    coughProbability = (coughProbability / audioData.length).clamp(0.0, 1.0);
    coughProbability +=
        (math.Random().nextDouble() - 0.5) * 0.2; // ¬±0.1 variation
    coughProbability = coughProbability.clamp(0.0, 1.0);

    print('üéµ Fallback audio analysis result: $coughProbability');
    return coughProbability;
  }

  double _deq(dynamic out, _QInfo qi) {
    double p;
    if (qi.type == TensorType.uint8) {
      final u8 = (out[0][0] as int);
      p = (u8 - qi.zeroPoint) * qi.scale;
    } else {
      p = (out[0][0] as double);
    }
    if (p < 0) p = 0;
    if (p > 1) p = 1;
    return p;
  }

  Uint8List _q2D(List<List<double>> t2, double scale, int zp) {
    final t = t2.length, c = t2[0].length;
    final out = Uint8List(t * c);
    int k = 0;
    for (int i = 0; i < t; i++) {
      for (int j = 0; j < c; j++) {
        final q = (t2[i][j] / scale + zp).round();
        out[k++] = q.clamp(0, 255);
      }
    }
    return out;
  }

  List _nest(Uint8List flat, List<int> shape) {
    final t = shape[1], c = shape[2];
    final nested = List.generate(
      1,
      (_) => List.generate(t, (_) => List<int>.filled(c, 0)),
    );
    int k = 0;
    for (int i = 0; i < t; i++) {
      for (int j = 0; j < c; j++) {
        nested[0][i][j] = flat[k++];
      }
    }
    return nested;
  }
}
